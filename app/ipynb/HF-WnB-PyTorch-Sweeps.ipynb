{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LddtUtSobNpO"
      },
      "source": [
        "# PoC Hugging Face, WandB, PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX1rsNd7eZWI"
      },
      "source": [
        "## Goal\n",
        "\n",
        "* Explorer possible pipeline\n",
        "* Use existing tools to focus on data and models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6QizarzASZ5"
      },
      "source": [
        "## Schema\n",
        "\n",
        "* SCM/VCS Github\n",
        "* Exploration with Jupyter Notebooks\n",
        "* Models, datasets, tokenizer, metrics from Hugging Face\n",
        "* Logging und visualisation with Weights&Biases (WandB)\n",
        "\n",
        "![Pipeline Jupyter HF WandB](https://raw.githubusercontent.com/qte77/ML-HF-WnB/32f21d112ab707b737a07cd027ad837b680f32fe/img/ML-Pipeline-HF-WnB.draw.io.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szPnwciqskcq"
      },
      "source": [
        "## TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSvVctO-sl1h"
      },
      "source": [
        "* Export helper functions for saving/loading into py\n",
        " * models, datasets, tokenizer, metrics\n",
        "* Import specific architecture with [PretrainedConfig](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/bert#transformers.BertConfig)\n",
        " * vocab_size, hidden_size, num_attention_heads, num_hidden_layers\n",
        "* Try multi processsing\n",
        " * python module `multiprocessing`\n",
        " * linux `!nohup`\n",
        "* Mount gdrive non-interactive, e.g. PyDrive\n",
        " * [Google loading and saving data from external sources](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=zU5b6dlRwUQk) \n",
        "* [HF How to benchmark models with Transformers\n",
        "](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)\n",
        " * deprecated, use other module/framework or self-implement\n",
        "* Use dataset specific metrics\n",
        " * [GLUE](https://github.com/huggingface/datasets/tree/master/metrics/glue), [SuperGLUE](https://github.com/huggingface/datasets/tree/master/metrics/super_glue), [SQuAD](https://github.com/huggingface/datasets/tree/master/metrics/squad), [SQuADv2](https://github.com/huggingface/datasets/tree/master/metrics/squad_v2)\n",
        "* Instead of [HF Metrics Builder Scripts](https://github.com/huggingface/datasets/tree/master/metrics), try\n",
        " * [from sklearn.metrics import precision_recall_fscore_support, accuracy_score]()\n",
        " * ` from datasets import load_metric`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqKxC-LPO2Y0"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlcTuMK4cRz"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "* [Paper: Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
        "* [Paper: BERT Bi-Directional Encoder Representation of Transformer](https://arxiv.org/abs/1810.04805)\n",
        "* The Annotated Transformer: [Artikel](https://nlp.seas.harvard.edu/2018/04/03/attention), [Github](https://github.com/harvardnlp/annotated-transformer)\n",
        "* [Tensorflow tutorial: Transformer model for language understanding](https://www.tensorflow.org/text/tutorials/transformer)\n",
        "* HF DistilBERT \"Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT\": [Paper](https://arxiv.org/abs/1910.01108), [Blog](https://medium.com/huggingface/distilbert-8cf3380435b5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh0Dm016kBQI"
      },
      "source": [
        "## DistilBERT Tips\n",
        "\n",
        "* DistilBERT doesn’t have token_type_ids, you don’t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token (or [SEP]).\n",
        "* DistilBERT doesn’t have options to select the input positions (position_ids input). This could be added if necessary though, just let us know if you need this option.\n",
        "\n",
        "\n",
        "[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG99YKXu4fAN"
      },
      "source": [
        "## Benchmarks\n",
        "\n",
        "* [GLUE General Language Understanding and Evaluation](https://gluebenchmark.com/)\n",
        "* [SuperGLUE]()\n",
        "* [SQuAD Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)\n",
        "* [Paper: Long Range Arena: A Benchmark for efficient Transformers](https://arxiv.org/abs/2011.04006)\n",
        "* [Paper: Efficient Transformers: A survey](https://arxiv.org/abs/2009.06732)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t0e7FNR4iIo"
      },
      "source": [
        "# Additional resources\n",
        "\n",
        "* [Are Sixteen Heads Really Better than One?](https://blog.ml.cmu.edu/2020/03/20/are-sixteen-heads-really-better-than-one/)\n",
        "* [HF How to benchmark models with Transformers\n",
        "](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)\n",
        "* [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)\n",
        "* [MetaAI OPT175B Logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf)\n",
        "* BigScience 176B multi-lingual\n",
        " * [Lessons learned](https://github.com/bigscience-workshop/bigscience/blob/master/train/lessons-learned.md)\n",
        " * [Chronicles](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md)\n",
        " * [TensorBoard](https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard)\n",
        " * [Paper](https://openreview.net/forum?id=rI7BL3fHIZq)\n",
        " * [Blog](https://bigscience.huggingface.co/blog/model-training-launched)\n",
        " * [Announcemenr](https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours)\n",
        "* [ML Roadmap 2020](https://whimsical.com/machine-learning-roadmap-2020-CA7f3ykvXpnJ9Az32vYXva)\n",
        "* WandB\n",
        " * [WandB get raw data](https://docs.wandb.ai/guides/track/public-api-guide)\n",
        "* Mixed Precision Training\n",
        " * [Paper: Mixed Precision Training](https://arxiv.org/pdf/1710.03740.pdf)\n",
        " * [Nvidia: Train With Mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)\n",
        " * [Fast.ai: Mixed precision training](https://docs.fast.ai/callback.fp16.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Yvni_4O-wa"
      },
      "source": [
        "# Pre-Requisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6osAQONz3Y_E"
      },
      "source": [
        "## Module-Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n3iT81Zkg-l"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx0baGb0sNjg"
      },
      "outputs": [],
      "source": [
        "red='\\033[31m'\n",
        "green='\\033[32m'\n",
        "orange='\\033[33m'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9SDuQYwa4rt"
      },
      "outputs": [],
      "source": [
        "# os.environ['req'] = \"https://raw.githubusercontent.com/qte77/ML-HF-WnB/main/k8s-app/app/config/\"\n",
        "# os.environ['rfn'] = \"requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecMhQaKq_6J9"
      },
      "outputs": [],
      "source": [
        "# %%shell\n",
        "# if [ ! -f $rfn ]; then\n",
        "#   echo \"Downloading ${rfn}\"\n",
        "#   curl \"${req}${rfn}\" -o $rfn\n",
        "# else\n",
        "#   echo \"${rfn} already in path\"\n",
        "# fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBEcrt-1f64K"
      },
      "outputs": [],
      "source": [
        "# !{sys.executable} -m pip install -r $rfn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZDfvGMhDaf7"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install -qqq setuptools watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5_Kv3AGLv0j"
      },
      "outputs": [],
      "source": [
        "#pre-install folium because wandb-version is obsolete\n",
        "!{sys.executable} -m pip  install -qqq 'folium == 0.2.1'\n",
        "!{sys.executable} -m pip install -qqq wandb\n",
        "#remove and re-install folium from wandb if pre-install fails\n",
        "#!{sys.executable} -m pip uninstall -yyy -qqq folium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJYZTup5C_JU"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install -qqq datasets transformers\n",
        "# Optional -> install latest version from source\n",
        "#!{sys.executable} -m pip install -qqq git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV6P-DK520jC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import watermark\n",
        "%load_ext watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pgsswo6NkDb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, list_datasets, list_metrics\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_metric\n",
        "import wandb\n",
        "#load bert_score if needed\n",
        "#import bert_score\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GocxY0TNoBkk"
      },
      "outputs": [],
      "source": [
        "%watermark -a qte77 -gu qte77 -ws qte77.github.io -u -i -v -iv\n",
        "#%watermark?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvWRgS9M1LtR"
      },
      "source": [
        "## Mount storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76xtJcYDnJIY"
      },
      "source": [
        "### interactive mount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Pk3IQrWib2"
      },
      "source": [
        "Mount non-interactivelly not possible\n",
        "\n",
        "* https://github.com/googlecolab/colabtools/issues/2563#issuecomment-1083524007\n",
        "\n",
        "!gcsfuse\n",
        "\n",
        "* https://cloud.google.com/storage/docs/gcs-fuse\n",
        "* https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md\n",
        "\n",
        "!gcloud --help --no-browser --access-token-file {conf_dir}/{keyfile}\n",
        "\n",
        "* https://cloud.google.com/sdk/docs/cheatsheet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LofAiiLW1RoG"
      },
      "outputs": [],
      "source": [
        "gdrive = '/gdrive'\n",
        "save_dir = f'{gdrive}/MyDrive' #no spaces allowed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCyvsl754wfp"
      },
      "outputs": [],
      "source": [
        "drive.mount(gdrive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3juvgGaJr1l-"
      },
      "source": [
        "## Parametrise experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISXMkdQgz7QW"
      },
      "source": [
        "* Model\n",
        "* Dataset\n",
        "* Metrics\n",
        " * Primary metric for eval\n",
        " * Further metrics to use\n",
        "* WandB\n",
        "  * Project (entity)\n",
        "  * Logging settings\n",
        "* Compute accelerator (CPU, GPU, TPU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOo0lkS6PfqD"
      },
      "outputs": [],
      "source": [
        "dataset = 'mrpc'\n",
        "model = 'rbc'\n",
        "wnb_entity = 'ba'\n",
        "wnb_run_group = '' #'no-label-smoothing-more-steps'\n",
        "wnb_job_type = 'training'\n",
        "wnb_notes = 'Runs without label_smoothing changed and more steps'\n",
        "wnb_tags = ['medium-range', 'no-label-smoothing']\n",
        "train_count = '5'\n",
        "metric_to_optimize = 'f1'\n",
        "#https://huggingface.co/metrics\n",
        "metrics_to_load = ['accuracy', 'precision', 'recall', 'f1', 'mae', 'mse']\n",
        "toggle_reproduce_wrong_optim: bool = False\n",
        "nvidia_smi_query = 'timestamp,name,temperature.gpu,utilization.gpu,' \\\n",
        "      'utilization.memory,memory.total,memory.free,memory.used'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axH0ihTWPhYe"
      },
      "source": [
        "## Validate and ingest parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-G70KObL3Sc"
      },
      "outputs": [],
      "source": [
        "#https://github.com/huggingface/datasets\n",
        "#dataset/task, configuration (sub ds/task), col to rename for tokenizer, cat avg for f1/recall/prec,\n",
        "#MRPC human annotations for whether the sentences in the pair are semantically equivalent\n",
        "#https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv\n",
        "#MNLI Multi-Genre Natural Language Inference Corpus, sentence pairs\n",
        "#MNLI https://dl.fbaipublicfiles.com/glue/data/MNLI.zip \n",
        "dataset_param = {\n",
        "    'YAHOO': ['yahoo_answers_topics','','macro','topic',['question_title'], #,'question_content','best_answer'],\n",
        "              ['id','question_content','best_answer']],\n",
        "    'MRPC': ['glue','mrpc','macro','label',['sentence1','sentence2'],['idx']],\n",
        "    'MNLI': ['glue','mnli','macro','label',['premise','hypothesis'],['idx']]\n",
        "}\n",
        "model_param = {\n",
        "    'DBBU' : 'distilbert-base-uncased',\n",
        "    'BBU' : 'bert-base-uncased',\n",
        "    #https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerForSequenceClassification.forward\n",
        "    'LBU' : 'allenai/longformer-base-4096',\n",
        "    'ESG' : 'google/electra-small-generator',\n",
        "    'ESD' : 'google/electra-small-discriminator',\n",
        "    'EBD' : 'google/electra-base-discriminator',\n",
        "    'ABU1' : 'albert-base-v1',\n",
        "    'ABU2' : 'albert-base-v2',\n",
        "    'RBC' : 'roberta-base',\n",
        "}\n",
        "params = {\n",
        "  'accuracy' : ['maximize', True],\n",
        "  'f1' : ['maximize', True]  ,\n",
        "  'loss' : ['minimize', False],\n",
        "  'eval_loss' : ['minimize', False]\n",
        "}\n",
        "'''\n",
        "try:\n",
        "  metrics_to_load.index(metric_to_optimize)\n",
        "except:\n",
        "  print('Metric to optimize not contained in metrics to load.')\n",
        "'''\n",
        "try:\n",
        "  dataset = dataset.upper()\n",
        "  ds_name, ds_config, ds_avg, ds_colren, ds_colstokens, ds_colrem = dataset_param.get(dataset)\n",
        "except Exception as e:\n",
        "  print(red, e)\n",
        "try:\n",
        "  model = model.upper()\n",
        "  modelname = model_param[model]\n",
        "except Exception as e:\n",
        "  print(red, e)\n",
        "try:\n",
        "  metric_to_optimize.lower()\n",
        "  goal, greaterBool = params.get(metric_to_optimize, ['Invalid metric.',''])\n",
        "except Exception as e:\n",
        "  print(red, e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12ieiqO3AQQn"
      },
      "outputs": [],
      "source": [
        "#os.environ['COLAB_TPU_ADDR']\n",
        "#os.environ['XRT_TPU_CONFIG']\n",
        "try:\n",
        "  os.environ['TPU_NAME']\n",
        "  device = 'tpu'\n",
        "except:\n",
        "  try:\n",
        "    #torch.cuda.is_available()\n",
        "    #cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, ve, ort, mlc, xla, lazy, vulkan, meta, hpu\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  except Exception as e:\n",
        "    print(red, e)\n",
        "device = f'{device}'.upper()\n",
        "print(device)\n",
        "if f'{device}' == 'CUDA':\n",
        "  %shell nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeZYnmM0-hDj"
      },
      "outputs": [],
      "source": [
        "wnb_project_name=f'{model}-{dataset}-{device}-sweep'\n",
        "print(green, wnb_project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXPJYNSjDU40"
      },
      "outputs": [],
      "source": [
        "#https://docs.wandb.ai/guides/track/advanced/environment-variables\n",
        "%env WANDB_WATCH=all\n",
        "%env WANDB_LOG_MODEL=true\n",
        "%env WANDB_SAVE_CODE=true\n",
        "%env WANDB_PROJECT={wnb_project_name}\n",
        "%env WANDB_ENTITY={wnb_entity}\n",
        "# %env WANDB_JOB_TYPE={wnb_job_type}\n",
        "# %env WANDB_RUN_GROUP={wnb_run_group}\n",
        "%env WANDB_NOTES={wnb_notes}\n",
        "%env WANDB_TAGS=wnb_tags\n",
        "#avoid error:\n",
        "#The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "%env TOKENIZERS_PARALLELISM=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn2vru-yDmZv"
      },
      "outputs": [],
      "source": [
        "#os.environ['COLAB_GPU']\n",
        "#os.environ['COLAB_TPU_ADDR']\n",
        "#os.environ['XRT_TPU_CONFIG']\n",
        "#os.environ['TPU_NAME']\n",
        "#os.environ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIYKHTtYhKDK"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQivzjZ-cyC9"
      },
      "source": [
        "## Links\n",
        "\n",
        "* [HF Datasets](https://huggingface.co/docs/datasets/index)\n",
        "* [Load](https://huggingface.co/docs/datasets/load_hub)\n",
        "* [EDA](https://huggingface.co/docs/datasets/access)\n",
        "* [Pre-process]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEwBLTCvz42D"
      },
      "source": [
        "## Load and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc0RJ9JlL3St"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#https://huggingface.co/docs/datasets/v1.2.1/loading_datasets.html\n",
        "#https://huggingface.co/docs/datasets/loading#local-and-remote-files\n",
        "#list_datasets()\n",
        "#load_ds also splits into train/eval\n",
        "\n",
        "dataset_dir = f'{save_dir}/Datasets/{dataset}'\n",
        "\n",
        "if ds_config == '':\n",
        "  print(orange, f'Downloading \"{ds_name}\"')\n",
        "  ds = load_dataset(ds_name)\n",
        "else:\n",
        "  print(orange, f'Downloading \"{ds_config}\" from \"{ds_name}\"')\n",
        "  ds = load_dataset(ds_name, ds_config)\n",
        "\n",
        "ds.save_to_disk(dataset_dir)\n",
        "\n",
        "#TODO save and load locally\n",
        "# dataset_dir = f'{save_dir}/Datasets/{dataset}'\n",
        "# try:\n",
        "#   if not os.path.exists(dataset_dir):\n",
        "#     os.makedirs(dataset_dir)\n",
        "#     if ds_config == '':\n",
        "#       print(orange, f'Downloading and saving dataset \"{ds_name}\".')\n",
        "#       ds = load_dataset(ds_name)\n",
        "#     else:\n",
        "#       print(orange, f'Downloading and saving dataset \"{ds_config}\" from \"{ds_name}\".')\n",
        "#       ds = load_dataset(ds_name, ds_config)\n",
        "#     ds.save_to_disk(dataset_dir)\n",
        "#   else:\n",
        "#     print(orange, f'Loading dataset from {dataset_dir}')    \n",
        "#     # if ds_config == '':\n",
        "#     data_files = { 'train': 'train' }\n",
        "#     ds = load_dataset(path = dataset_dir, data_files = data_files)\n",
        "#     # else:\n",
        "#     #   ds = load_dataset(path=ds_name, name=ds_config, data_files=dataset_dir)\n",
        "# except Exception as e:\n",
        "#   print(red, e)\n",
        "\n",
        "print(ds_name, ds_config, dataset_dir)\n",
        "\n",
        "# dataset_dir = None; del dataset_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFZtN4ghFqjV"
      },
      "outputs": [],
      "source": [
        "%shell ls -ARsh /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL2LJfrBg9HC"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuO2dYAOe7hQ"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3Yk2l5ncaZf"
      },
      "outputs": [],
      "source": [
        "ds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxnCiKfrcHV0"
      },
      "outputs": [],
      "source": [
        "ds['train'][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_D3GipqfUqM"
      },
      "outputs": [],
      "source": [
        "lblcnt0 = ds['train']['label'].count(0)\n",
        "lblcnt1 = ds['train']['label'].count(1)\n",
        "print(f'Label count: 0x {lblcnt0}, 1x {lblcnt1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLq-OLDg5Y8R"
      },
      "outputs": [],
      "source": [
        "# from sklearn.utils import compute_class_weight\n",
        "# classWeight = compute_class_weight(\n",
        "#   'balanced',\n",
        "#   classes = ds['train']['labels'],\n",
        "#   y = ds['train']['sentence1']\n",
        "# ) \n",
        "# classWeight = dict(enumerate(classWeight))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIDYeTowhERx"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvP6E4e4YeTK"
      },
      "outputs": [],
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrRvN4dP_Idd"
      },
      "outputs": [],
      "source": [
        "label_list = ds['train'].unique(ds_colren)\n",
        "num_labels = len(label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uae8nnd_EQA"
      },
      "outputs": [],
      "source": [
        "#rename column 'dscol_rename', model expects 'labels'\n",
        "for name in ds:\n",
        "  if ds_colren in ds[name].column_names:\n",
        "    ds[name] = ds[name].rename_column(ds_colren, 'labels')\n",
        "  else:\n",
        "    print(red, \"Attribute/Feature/Column '%s' not found in '%s'. Found:\" % (ds_colren, name))\n",
        "    print(ds[name].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My7toWl8_CEr"
      },
      "outputs": [],
      "source": [
        "ds.column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUmFTfF7YYw5"
      },
      "source": [
        "## Load and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9APXd1uO8ZVQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# tokenizer converts the tokensto vocabulary indices and pads batched data\n",
        "#TODO try args max_length=X and fast=False\n",
        "tokenizer_dir = f'{save_dir}/Tokenizer/{modelname}'\n",
        "try:\n",
        "  if not os.path.exists(tokenizer_dir):\n",
        "    print(orange, f'Downloading and saving tokenizer to {tokenizer_dir}')\n",
        "    os.makedirs(tokenizer_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(modelname, use_fast=True, truncation=True, padding=True)\n",
        "    tokenizer.save_pretrained(tokenizer_dir)\n",
        "  else:\n",
        "    print(orange, f'Loading tokenizer from {tokenizer_dir}')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
        "except Exception as e:\n",
        "  print(red, e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHhXbHTq5qff"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"This is not Sparta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmCgdV4T6Ugn"
      },
      "outputs": [],
      "source": [
        "print(\"Vocab size: %s\" % len(tokenizer.vocab))\n",
        "print(\"Special tokens: %s\" % tokenizer.special_tokens_map.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLecvlyS_P3U"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#tokenizing and padding\n",
        "#try tokenizer(padding=\"max_length\", max_length=X)\n",
        "#attributes needed for down-stream training:\n",
        "#'input_ids', 'token_type_ids', 'attention_mask', 'labels'\n",
        "#lambda convert tuple to list, list comprehension?\n",
        "#'labels' has to be present after tokenization to avoid KeyError('loss')\n",
        "#https://github.com/huggingface/transformers/issues/11256\n",
        "'''\n",
        "#Lambda with input_cols and list comprehension, but *x passes tuples\n",
        "ds_tokenized = ds.map(\n",
        "    lambda *x: a=tokenizer([item for item in x], truncation=True), #t in x for item in t\n",
        "    input_columns=ds_colstokens, batched=True\n",
        "  )\n",
        "# explicit lambda with case\n",
        "ds_tokenized = ds.map(lambda x: tokenizer(x[\"sentence1\"],x[\"sentence2\"],truncation=True),batched=True)\n",
        "#function overloading with input_cols, dataset not passed to fun by map()\n",
        "def tokenize(col1, col2=None, col3=None):\n",
        "  #...\n",
        "ds_tokenized = ds.map(tokenize, input_columns=ds_colstokens)\n",
        "'''\n",
        "\n",
        "def tokenize(data):\n",
        "  cols = []\n",
        "  for v in ds_colstokens:\n",
        "    cols.append(data[v])\n",
        "  #TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
        "  return tokenizer(cols, truncation=True)\n",
        "  #Error ArrowInvalid: Column 4 named input_ids expected length 1000 but got length 2\n",
        "  #with is_split_into_words=True\n",
        "\n",
        "def tokenize(data):\n",
        "  colen = len(ds_colstokens)\n",
        "  if colen == 3:\n",
        "    return tokenizer(data[ds_colstokens[0]], data[ds_colstokens[1]], data[ds_colstokens[2]], truncation=True)\n",
        "  elif colen == 2:\n",
        "    return tokenizer(data[ds_colstokens[0]], data[ds_colstokens[1]], truncation=True)\n",
        "  elif colen == 1:\n",
        "    return tokenizer(data[ds_colstokens[0]], truncation=True)\n",
        "\n",
        "ds_tokenized = ds.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u2pd3cPW5ma"
      },
      "outputs": [],
      "source": [
        "#avoid info 'The following columns in the evaluation set  don't have a corresponding argument'\n",
        "try:\n",
        "  ds_tokenized = ds_tokenized.remove_columns(ds_colstokens).remove_columns(ds_colrem)\n",
        "except Exception as e:\n",
        "  print(red, e)\n",
        "ds_tokenized.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCdEh83fegeo"
      },
      "outputs": [],
      "source": [
        "#save tokenized dataset to /root/.cache/huggingface/datasets\n",
        "#or google drive, mount beforehand\n",
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPMiV6gYjl2S"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1DDJkLxYGqM"
      },
      "source": [
        "## Load and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVbYZChVPTNN"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "model_dir = f'{save_dir}/Models/{modelname}'\n",
        "try:\n",
        "  if not os.path.exists(model_dir):\n",
        "    print(green, f'Downloading and saving model to {model_dir}')\n",
        "    os.makedirs(model_dir)\n",
        "    modelobj = AutoModelForSequenceClassification.from_pretrained(modelname, num_labels=num_labels)\n",
        "    modelobj.save_pretrained(model_dir)\n",
        "  else:\n",
        "    print(green, f'Loading model from {model_dir}')\n",
        "    modelobj = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "except Exception as e:\n",
        "  print(red, e)\n",
        "\n",
        "model_dir = None; del model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCnHgc38DtaP"
      },
      "source": [
        "## Pre-trained hyperparam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwM99BeHCypY"
      },
      "outputs": [],
      "source": [
        "print(modelobj.config)\n",
        "# modelobj.config.classifier_dropout = 0.2\n",
        "# modelobj.config.max_position_embeddings = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzD1LUD2C6Ds"
      },
      "source": [
        "## Transformer architecture\n",
        "\n",
        "![Transformer Attention](https://miro.medium.com/max/875/1*9nUzdaTbKzJrAsq1qqJNNA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-V2vkWeGOKE"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4doME5oDQu9"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  print(modelobj.base_model.encoder.layer[0])\n",
        "except:\n",
        "  try:\n",
        "    print(modelobj.base_model.transformer.layer[0])\n",
        "  except:\n",
        "    try:\n",
        "      print(modelobj.base_model.encoder)\n",
        "    except Exception as e:\n",
        "      print(red, e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXVsfAvnFRtc"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daRgWIk1FQbQ"
      },
      "outputs": [],
      "source": [
        "print(modelobj.base_model.embeddings)\n",
        "# print(modelobj.bert.embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXtDG5RP1vsQ"
      },
      "source": [
        "## Test model before fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBqBNxk6PZRw"
      },
      "outputs": [],
      "source": [
        "#TODO source for test function, WandB colab?\n",
        "#TODO multi inputs\n",
        "def test_model(inputs, tokenizer=tokenizer, model=modelobj):\n",
        "  if device == \"CUDA\":\n",
        "    for i in inputs:\n",
        "      print(i)\n",
        "    # inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    # ensure model and inputs are on the same device (GPU)\n",
        "    # inputs = {name: tensor.cuda() for name, tensor in inputs.items()}\n",
        "    # model = model.cuda()\n",
        "    # get prediction - 10 classes \"probabilities\" (not really true because they still need to be normalized)\n",
        "    # with torch.no_grad(): \n",
        "    #     predictions = model(**inputs)[0].cpu().numpy()\n",
        "    # get the top prediction class and convert it to its associated label\n",
        "    # top_prediction = predictions.argmax().item()\n",
        "    # return ds['train'].features['labels'].int2str(top_prediction)\n",
        "    return 1\n",
        "  else:\n",
        "    return \"NO CUDA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKdaDnUlPaqd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if dataset == 'YAHOO':\n",
        "  print(test_model('Why is cheese so much better with wine?'))\n",
        "# elif dataset == 'MRPC':\n",
        "#   print(test_model('hallo', 'hedda'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTmgz_w1KTWx"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMsVZQU-dOqs"
      },
      "source": [
        "## Links\n",
        "\n",
        "* [Metrics (deprecated)](https://huggingface.co/metrics)\n",
        "* [Evaluate (new)](https://huggingface.co/docs/evaluate/index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBAuIZN_dbie"
      },
      "source": [
        "## Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdgCvkupu2FW"
      },
      "source": [
        "### Model\n",
        "\n",
        "* Loss (MSE, MAE)\n",
        "* Accuracy\n",
        "* Recall, Precision, F1\n",
        "* Perplexity (PPL)\n",
        "* BLEU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT7jlMadu3tF"
      },
      "source": [
        "### System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NToitSVpuYAT"
      },
      "source": [
        "## PrecisionRecallCurve\n",
        "* [sklearn.metrics.precision_recall_curve](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Plot_Precision_Recall_Curves_with_W%26B.ipynb)\n",
        "* [plot_precision_recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py)\n",
        "* [plot_display_object_visualization](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSOOPR1I5CaK"
      },
      "source": [
        "## Load with HF Metrics Builder Scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJbMeYSnGuvf"
      },
      "outputs": [],
      "source": [
        "# #list_metrics()\n",
        "# metrics_dir = f'{save_dir}/Metrics'\n",
        "# metrics_loaded = []\n",
        "\n",
        "# #downloading metrics builder scripts\n",
        "# try:\n",
        "#   for met in metrics_to_load:\n",
        "#     print(orange, f'Downloading builder script for \"{met}\".')\n",
        "#     metrics_loaded.append(load_metric(met))\n",
        "#     print(green, metrics_loaded[-1].description)\n",
        "#     print(green, metrics_loaded[-1].features)\n",
        "#     print('\\n')\n",
        "# except Exception as e:\n",
        "#   print(red, e)\n",
        "# #TODO save locally\n",
        "# # for met in metrics_loaded:\n",
        "#   # met."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIUNUKgHuU5V"
      },
      "outputs": [],
      "source": [
        "# def compute_metrics(eval_pred):\n",
        "#   predictions, labels = eval_pred\n",
        "#   predictions = np.argmax(predictions, axis=1) #predictions.argmax(-1)\n",
        "  \n",
        "#   print(orange,\"*************\")\n",
        "  \n",
        "#   for i, m in enumerate(metrics_loaded):\n",
        "\n",
        "#     if metrics_to_load[i] in ['precision','recall','f1']:\n",
        "#       met = m.compute(predictions=predictions, references=labels, average=ds_avg)\n",
        "#     else:\n",
        "#       met = m.compute(predictions=predictions, references=labels)\n",
        "\n",
        "#     if metrics_to_load[i] == 'accuracy':\n",
        "#       ret = met\n",
        "\n",
        "#     wandb.log(met)\n",
        "#     print(met)\n",
        "\n",
        "#     #test if metrics-obj need to be reloaded to avoid same eval values\n",
        "#     #metrics_loaded[i] = load_metric(metrics_to_load[i])\n",
        "\n",
        "# #    print(\"**************** dir(predictions)\")\n",
        "# #    print(dir(met))\n",
        "# #    print(\"**************** dir(labels)\")\n",
        "# #    print(dir(labels))\n",
        "# #    print(\"**************** m.__dir__\")\n",
        "# #    print(m.__dict__)\n",
        "# #    print(\"**************** dir(m)\")\n",
        "# #    print(dir(m))\n",
        "    \n",
        "#   print(orange,\"*************\")\n",
        "\n",
        "#   return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcygp_j9uHxR"
      },
      "source": [
        "## Load with specific HF Builder Scripts for provided dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwpDxsHIuHTw"
      },
      "outputs": [],
      "source": [
        "#dataset specific builder scripts by Hf\n",
        "from datasets import load_metric\n",
        "if ds_config == None:\n",
        "  metrics = load_metric(ds_name)\n",
        "else:\n",
        "  metrics = load_metric(ds_name, ds_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFFHSFsZLPN2"
      },
      "outputs": [],
      "source": [
        "metrics_to_load = [\"recall\", \"precision\", \"mse\", \"mae\"]\n",
        "metrics_loaded = []\n",
        "met_avg = \"macro\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkKCoXqQLatZ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  for met in metrics_to_load:\n",
        "    print(orange, f'Downloading builder script for \"{met}\".')\n",
        "    metrics_loaded.append(load_metric(met))\n",
        "    # print(green, metrics_loaded[-1].description)\n",
        "    # print(green, metrics_loaded[-1].features)\n",
        "    # print('\\n')\n",
        "except Exception as e:\n",
        "  print(red, e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blXPQMN_vAw_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "\n",
        "  #TODO, same axis as np?\n",
        "  #https://numpy.org/doc/stable/reference/generated/numpy.argmax.html\n",
        "  predictions = np.argmax(pred.predictions, axis=1)\n",
        "  # predictions = pred.predictions.argmax(-1)\n",
        "  labels = pred.label_ids\n",
        "\n",
        "  results = metrics.compute(\n",
        "      predictions = predictions,\n",
        "      references = labels\n",
        "  )\n",
        "\n",
        "  for i, metric in enumerate(metrics_loaded):\n",
        "    if metrics_to_load[i] in ['precision','recall']:\n",
        "      met = metric.compute(\n",
        "        predictions = predictions,\n",
        "        references = labels,\n",
        "        average = met_avg\n",
        "      )\n",
        "    else:\n",
        "      met = metric.compute(\n",
        "        predictions = predictions,\n",
        "        references = labels\n",
        "      )\n",
        "    results[metrics_to_load[i]] = met[metrics_to_load[i]]\n",
        "\n",
        "  print(orange,\"*************\")\n",
        "  wandb.log(results)\n",
        "  print(results)\n",
        "  \n",
        "  if device == 'CUDA':\n",
        "    #https://developer.nvidia.com/nvidia-system-management-interface\n",
        "    #https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n",
        "    %shell nvidia-smi --query-gpu={nvidia_smi_query} --format=csv\n",
        "\n",
        "  print(orange,\"*************\")\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbM-d3z6uAQM"
      },
      "source": [
        "## Load with sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L6263Ykh2Mm"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "# def compute_metrics(pred):\n",
        "#     \"\"\"\n",
        "#     Compute metrics for Trainer\n",
        "#     \"\"\"\n",
        "#     labels = pred.label_ids\n",
        "#     preds = pred.predictions.argmax(-1)\n",
        "#     #_, _, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
        "    \n",
        "#     acc = accuracy_score(labels, preds)\n",
        "\n",
        "#     print(f'acc: {acc}')\n",
        "#     print(f'f1: {f1}')\n",
        "\n",
        "#     print(orange,\"*************\")\n",
        "#     wandb.log(met)\n",
        "#     print(met)\n",
        "#     print(orange,\"*************\")\n",
        "\n",
        "#     return {\n",
        "#         'accuracy': acc,\n",
        "#         'f1': f1,\n",
        "#         #'macro f1': macro_f1,\n",
        "#         'precision': precision,\n",
        "#         'recall': recall\n",
        "#     }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nAfgDi1hbid"
      },
      "source": [
        "# Sweep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaKUEAg6iiqP"
      },
      "source": [
        "## Links\n",
        "* [Sweep configuration](https://docs.wandb.com/sweeps/configuration)\n",
        "* [YAML file](https://docs.wandb.com/sweeps/quickstart#2-sweep-config)\n",
        "* [sweep random variables](https://docs.wandb.com/sweeps/configuration#distributions)\n",
        "* [simple training script and a few flavors of sweep configs](https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion)\n",
        "* [list of all configuration options](https://docs.wandb.com/library/sweeps/configuration)\n",
        "* [Big collection of examples in YAML format](https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion)\n",
        "* [Sweep from an existing project](https://docs.wandb.ai/guides/sweeps/existing-project)\n",
        "* [Sweep on CLI Quickstart](https://docs.wandb.com/sweeps/quickstart)\n",
        "* [Example Tune Sweep Dashboard](https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/xbs2wm5e?workspace=user-lavanyashukla)\n",
        "* We offer to `early_terminate` your runs with the [HyperBand](https://arxiv.org/pdf/1603.06560.pdf) scheduling algorithm. See more [here](https://docs.wandb.com/sweeps/configuration#stopping-criteria)\n",
        "* [Bayesian BOHB ArXiv 1807.01774](https://arxiv.org/abs/1807.01774)\n",
        "* [Bayesian Hyperband](https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/us0ifmrf?workspace=user-lavanyashukla)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlfiiE24hgbv"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzwrNlnxL3Sg"
      },
      "outputs": [],
      "source": [
        "#get better acc\n",
        "if not toggle_reproduce_wrong_optim:\n",
        "  parameters_dict = {}\n",
        "  parameters_dict = {\n",
        "        'max_steps' : {\n",
        "          'values': list(range(4000,7000,1000))\n",
        "        },\n",
        "        'per_device_train_batch_size': {\n",
        "          'values' : [8, 16, 32] #not enough RAM: [64,128,256,512,1024]\n",
        "        },\n",
        "        # 'per_device_eval_batch_size':\n",
        "        # 'per_gpu_eval_batch_size':\n",
        "        # 'per_gpu_train_batch_size':\n",
        "        'seed' : {\n",
        "          # 'value': 101\n",
        "          'distribution': 'int_uniform',\n",
        "          'min': 1,\n",
        "          'max': 101\n",
        "        },\n",
        "        'evaluation_strategy': {\n",
        "          'distribution': 'categorical',\n",
        "          'values': ['steps', 'epoch'] #, 'no']          \n",
        "        },\n",
        "        # 'label_smoothing_factor': {\n",
        "        #   'distribution': 'uniform',\n",
        "        #   'min': 0,\n",
        "        #   'max': 0.1\n",
        "        # },\n",
        "  }\n",
        "  # parameters_dict['per_gpu_eval_batch_size'] = parameters_dict['per_device_train_batch_size']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p78-SZQJOs_k"
      },
      "outputs": [],
      "source": [
        "# #reproduce leveling out of metrics with wrong optimizers\n",
        "# #https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n",
        "if toggle_reproduce_wrong_optim:\n",
        "  parameters_dict = {}\n",
        "  parameters_dict = {\n",
        "      'learning_rate': {\n",
        "          'distribution': 'uniform',\n",
        "          'min': 5e-7,\n",
        "          'max': 5e-3\n",
        "        },\n",
        "        'seed' : {\n",
        "          'distribution': 'int_uniform',\n",
        "          'min': 51,\n",
        "          'max': 202\n",
        "        },\n",
        "        'max_steps' : {\n",
        "          'values': list(range(1500,5000,500))\n",
        "        },\n",
        "        'optim': {\n",
        "          'distribution': 'categorical',\n",
        "          'values': ['torch.AdamW', 'adafactor']# 'sgd', deprecated: adamw_hf\n",
        "        },\n",
        "        'evaluation_strategy': {\n",
        "          'value': 'steps'          \n",
        "        },\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5ci0ecXOpNC"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "  'method' : 'random' #grid, bayes\n",
        "}\n",
        "sweep_config['metric'] = {\n",
        "    'name': metric_to_optimize,\n",
        "    'goal': goal\n",
        "    }\n",
        "sweep_config['parameters'] = parameters_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rlKLP9YUuc_"
      },
      "outputs": [],
      "source": [
        "# print({\n",
        "#     'train': parameters_dict['per_device_train_batch_size']\n",
        "# })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw2E3JFEkdMA"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLtJN2wclUKN"
      },
      "source": [
        "* Die Funktion `train()` initiert die jeweiligen Durchläufe mit den Werten der Konfiguration, die der `wand.agent` auswählt\n",
        "* [**`wandb.init()`**](https://docs.wandb.com/library/init) – Initialize a new W&B Run. Each Run is a single execution of the training function\n",
        "* [**`wandb.config`**](https://docs.wandb.com/library/config) – Save all your hyperparameters in a configuration object so they can be logged. Read more about how to use `wandb.config` [here](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb)\n",
        "* More details on instrumenting W&B with PyTorch, see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkJSYkTUMFaO"
      },
      "source": [
        "## Args and Train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3AHEf9ZRoyK"
      },
      "outputs": [],
      "source": [
        "# num_train_epochs=16,              # total number of training epochs\n",
        "# per_device_train_batch_size=32,  # batch size per device during training\n",
        "# per_device_eval_batch_size=32,   # batch size for evaluation\n",
        "# warmup_steps=600,                # number of warmup steps for learning rate scheduler\n",
        "# weight_decay=0.01,               # strength of weight decay\n",
        "# logging_dir='/content/logs',\n",
        "# fp16 = True,  \n",
        "def train(config=None):\n",
        "  \n",
        "  with wandb.init(config=config):\n",
        "      \n",
        "    config = wandb.config\n",
        "\n",
        "    eval_steps = round(config.max_steps / 5)\n",
        "    save_steps = eval_steps * 2\n",
        "\n",
        "    #args need to be assigned here to avoid wandb runtime TypeError()\n",
        "    #\"'TrainingArguments' object does not support item assignment\"\n",
        "    args = TrainingArguments(\n",
        "      report_to = 'wandb',\n",
        "      output_dir = wnb_project_name,\n",
        "      run_name = wnb_project_name,\n",
        "      overwrite_output_dir = True,\n",
        "      load_best_model_at_end = True,\n",
        "      logging_steps = 100,\n",
        "      eval_steps = eval_steps,\n",
        "      save_steps = save_steps,\n",
        "      #remove_unused_columns = True,          # avoid info 'The following columns in the evaluation set  don't have a corresponding argument'\n",
        "      metric_for_best_model = metric_to_optimize,\n",
        "      greater_is_better = greaterBool,\n",
        "      #sweep params to be changed by sweep agent\n",
        "      max_steps = config.max_steps,\n",
        "      seed = config.seed,\n",
        "      #In Trainer, evaluation_strategy defaults to no, but save_strategy defaults to steps. Why? #14051\n",
        "      #https://github.com/huggingface/transformers/issues/14051\n",
        "      evaluation_strategy = config.evaluation_strategy,\n",
        "      save_strategy = config.evaluation_strategy,\n",
        "      # label_smoothing_factor = config.label_smoothing_factor,\n",
        "      per_device_train_batch_size = config.per_device_train_batch_size,\n",
        "      per_device_eval_batch_size = config.per_device_train_batch_size,\n",
        "    )\n",
        "\n",
        "    if toggle_reproduce_wrong_optim:\n",
        "      #['adamw_hf', 'adamw_torch', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor', 'adamw_bnb_8bit', 'sgd', 'adagrad']\n",
        "      #'adamw_hf' deprecated\n",
        "      args.optim = config.optim\n",
        "      #lr the lower the better in experiments\n",
        "      args.learning_rate = config.learning_rate\n",
        "    \n",
        "    trainer = Trainer(\n",
        "      model = modelobj,\n",
        "      args = args, \n",
        "      train_dataset = ds_tokenized['train'],\n",
        "      eval_dataset = ds_tokenized['test'],\n",
        "      tokenizer = tokenizer,\n",
        "      compute_metrics = compute_metrics\n",
        "    )\n",
        "    \n",
        "    print(orange,\"*************\")\n",
        "    print(\"Model: %s, Architectures: %s\" % (\n",
        "        trainer.model.name_or_path,\n",
        "        trainer.model.config.architectures\n",
        "      )\n",
        "    )\n",
        "    print(\"Metric to optimize: %s, #Labels: %s, Avg: %s\" % (\n",
        "        args.metric_for_best_model,\n",
        "        num_labels,\n",
        "        ds_avg\n",
        "      )\n",
        "    )\n",
        "\n",
        "    if args.evaluation_strategy.value == 'steps':\n",
        "      print(\"eval_steps: %s, save_steps: %s\" % (eval_steps, save_steps))\n",
        "    print(orange,\"*************\")\n",
        "\n",
        "    trainer.evaluate()\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORLn5_ZMk1NF"
      },
      "source": [
        "# WandB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBmj-w_R3mEs"
      },
      "source": [
        "## Load key-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8BOk4A16VIl"
      },
      "outputs": [],
      "source": [
        "wandb_keyfile = 'conf/wandb.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NztOv6ON3l0b"
      },
      "outputs": [],
      "source": [
        "#get API-key from https://app.wandb.ai/authorize\n",
        "#get key from file and save to ENV\n",
        "with open(f\"{save_dir}/{wandb_keyfile}\", 'r') as j:\n",
        " data = json.loads(j.read())\n",
        " os.environ['WANDB_USERNAME'] = data['username']\n",
        " os.environ['WANDB_API_KEY'] = data['key'] #WANDB_KEY deprecated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIuRkkm5M-q"
      },
      "source": [
        "## Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIrrKTFvJgJZ"
      },
      "outputs": [],
      "source": [
        "#wandb.login()\n",
        "#wandb.init(project=wnb_project_name, entity=wnb_entity, save_code = True)\n",
        "#wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mut4jST3zRIk"
      },
      "outputs": [],
      "source": [
        "#read api-key from ENV, if not provided from interactive user input \n",
        "#!wandb login --relogin\n",
        "!wandb login --cloud $WANDB_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K4uVsOYlE1f"
      },
      "source": [
        "## Schema\n",
        "\n",
        "* Initieren des Sweep Controlers durch `wandb.sweep()` mit `sweep_config` und `project`\n",
        "* Sweep Controler gibt `sweep_id` zurück, mit der Agents durch `wandb.agent` initiert werden können\n",
        "* `wandb.sweep`in CLI durch `wandb sweep config.yaml` ersetzt\n",
        "* Aufruf von `wandb.finish()` in Notebook notwendig, um Agenten zu beenden\n",
        "\n",
        "<img src=\"https://i.imgur.com/zlbw3vQ.png\" alt=\"sweeps-diagram\" width=\"500\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0RJvgaYlA1F"
      },
      "source": [
        "## Initialise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l_hV53KzSN3"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=wnb_project_name, entity=wnb_entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaXMOFaU5Sus"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2sePTHOOd2K"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=train_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOLIu-tRxzA8"
      },
      "source": [
        "## Finish if inside NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUzD66zUk0Tn"
      },
      "outputs": [],
      "source": [
        "#explicitly call finish within notebooks\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLkVbk8VNx9T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PA-HF-WnB-PyTorch-Sweeps.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a2c38984cde13219f5762decb43eb3d0c702ceca3242c525e06c46c0c4b9d01c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
